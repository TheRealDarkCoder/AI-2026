{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b25187f",
   "metadata": {},
   "source": [
    "# Artificial Intelligence\n",
    "## Assignment 6 – Reinforcement Learning\n",
    "\n",
    "### Personal details\n",
    "\n",
    "* **Name:** Ahmed Jabir Zuhayr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ed9347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da37cb",
   "metadata": {},
   "source": [
    "In Assignment 1 we solved the problem of finding the shortest path from point A to point B. Now we will expand on this by placing our agent in a dynamic environment where it must navigate through obstacles and reach a goal while avoiding traps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b123e9be",
   "metadata": {},
   "source": [
    "### 6.1 – Dynamic Environments\n",
    "\n",
    "![escaperoom.png](escaperoom.png)\n",
    "\n",
    "<small>Image generated with ChatGPT</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde0d6b6",
   "metadata": {},
   "source": [
    "You will be working with an environment that can be conceptualized as an escape room where a humanoid robot is tasked with finding its way out as quickly as possible. The robot can move in four directions (up, down, left, right) and can carry a key that allows it to open doors.\n",
    "\n",
    "We can repurpose the `Grid` class from Assignment 1 to represent this environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc88ca6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■ ■ ■ ■ ■ ■ ■ ■ ■ ■ G ■ ■ ■ ■\n",
      "■ . @ . ■ ~ ~ ~ ■ . . . ■ . ■\n",
      "■ . . . ■ ~ ~ ~ ■ . . . . . ■\n",
      "■ . . . ■ ~ ~ ~ ■ . . . ■ . ■\n",
      "■ . . . ■ ■ — ■ ■ ■ — ■ ■ . ■\n",
      "■ . . . ■ . . . T . . . ■ . ■\n",
      "■ . . . | . T . . . T . ■ . ■\n",
      "■ . . . ■ . . . T . . . ■ . ■\n",
      "■ ■ . ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ . ■\n",
      "■ K . . . . . . . . . . . . ■\n",
      "■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■\n"
     ]
    }
   ],
   "source": [
    "from ex6_utils import Grid\n",
    "\n",
    "grid = Grid(xlim=13+2, ylim=9+2)\n",
    "grid.generate_nodes(carrying_key=False)\n",
    "grid.generate_nodes(carrying_key=True)\n",
    "grid.get_initial().current = True\n",
    "grid.visualize(agent=None, delay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd26814",
   "metadata": {},
   "source": [
    "The symbols used in the visualization are as follows:\n",
    "\n",
    "- `@`: current position of the agent\n",
    "- `G`: goal\n",
    "- `—` / `|`: door\n",
    "- `■`: wall\n",
    "- `K`: key\n",
    "- `~`: lava\n",
    "- `T`: trap\n",
    "- `.`: empty node\n",
    "\n",
    "Doors are locked until the agent picks up the key, which happens automatically when it is reached. Lava leads to instant death and restart. Traps turn into lava when stepped into and are non-deterministic: there is a 50/50 chance of the agent falling into the lava or managing to jump into a random neighbouring square to save themselves.\n",
    "\n",
    "In this problem our agent has to make decisions based on incomplete information. The agent only knows its current position and whether it is holding a key, as well as the properties of the current node (e.g. whether it is standing in lava). This type of problem is well-suited for **reinforcement learning**, where the agent explores its environment and learns through trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6715f2e4",
   "metadata": {},
   "source": [
    "### 6.2 – Q-Learning\n",
    "\n",
    "The basic idea in reinforcement learning is to learn a **policy** that maximizes the expected cumulative reward for an agent by interacting with its environment. Actions performed yield *rewards* that can be either positive or negative, and the agent learns to associate actions with their outcomes.\n",
    "\n",
    "**Q-learning** is a form of reinforcement learning that uses a table of *action-value pairs* or **Q-values** to estimate the expected utility of taking a given action in a given state. The values are updated using the following equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right)\n",
    "$$\n",
    "\n",
    "where\n",
    "- $s$  is the current state\n",
    "- $a$  is the action taken\n",
    "- $r$  is the immediate reward received\n",
    "- $s'$ is the next state\n",
    "- $a'$ is an action taken in the next state\n",
    "- $\\alpha$ is the *learning rate*\n",
    "- $\\gamma$ is the *discount factor*\n",
    "\n",
    "Q-learning is **model-free**, meaning it neither knows nor learns the underlying transition model of the environment (\"with probability $P$, taking an action $a$ in state $s$ leads to state $s'$\"). All the agent learns is the value of being in a given state and taking a specific action, based on the rewards it has previously received. This allows the agent to learn a policy for always taking actions that maximize its expected reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2692f0e7",
   "metadata": {},
   "source": [
    "**Task 1: Reward function (0.2 pt)**\n",
    "\n",
    "Let's start by defining a class to represent the Q-learning agent. Your task is to fill in suitable values in the `get_reward` function (replace the zeros where appropriate) which is used to receive feedback from ending up in specific states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3b92bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent():\n",
    "    def __init__(self, grid):\n",
    "        self.grid = grid\n",
    "        self.current_node = grid.get_initial()\n",
    "        self.previous_node = None\n",
    "        self.previous_action = None\n",
    "        self.has_key = False\n",
    "        self.total_rewards = 0\n",
    "    \n",
    "    def move(self, direction):\n",
    "        self.previous_node = self.current_node\n",
    "        self.previous_action = direction\n",
    "\n",
    "        next_node = self.current_node.get_neighbor(direction, self.has_key)\n",
    "\n",
    "        if not next_node or next_node.blocked:\n",
    "            # Invalid move\n",
    "            return self.get_reward(next_node)\n",
    "\n",
    "        # Trap logic\n",
    "        if next_node.trap:\n",
    "            next_node.lava = True\n",
    "            while random.random() < 0.5 and next_node.blocked:\n",
    "                next_node = next_node.get_neighbor(\"random\", self.has_key)\n",
    "\n",
    "        # Key pickup\n",
    "        if next_node.has_key:\n",
    "            self.has_key = True\n",
    "\n",
    "        # Door logic\n",
    "        if next_node.locked:\n",
    "            if self.has_key:\n",
    "                next_node.locked = False\n",
    "            else:\n",
    "                # Invalid move\n",
    "                return self.get_reward(next_node)\n",
    "\n",
    "        # Valid move\n",
    "        self.current_node.current = False\n",
    "        next_node.current = True\n",
    "        self.current_node = next_node\n",
    "        self.grid.current_node = next_node\n",
    "        return self.get_reward(next_node)\n",
    "    \n",
    "    def get_reward(self, node):\n",
    "        # ---------- YOUR CODE HERE ----------- #\n",
    "        reward = 0\n",
    "        if node is None or node.blocked or node.locked:\n",
    "            reward += -1\n",
    "        elif node.goal:\n",
    "            reward += 100\n",
    "        elif node.lava:\n",
    "            reward += -100\n",
    "        elif node.trap:\n",
    "            reward += -10\n",
    "        elif node.has_key:\n",
    "            reward += 0\n",
    "        return reward\n",
    "        # ---------- YOUR CODE HERE ----------- #\n",
    "\n",
    "    def reset(self, grid):\n",
    "        self.grid = grid\n",
    "        self.has_key = False\n",
    "        self.current_node = self.grid.get_initial()\n",
    "        self.previous_action = None\n",
    "        self.total_rewards = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159b9d21",
   "metadata": {},
   "source": [
    "**Task 2: Q-Learning Update (0.5 pt)**\n",
    "\n",
    "Now let's bring in learning. Your task is to fill in the `update_q_value` function to update the Q-values based on the agent's experiences. You may experiment with different values for the learning rate and the discount factor, but reasonable defaults are $\\alpha = 0.5$ and $\\gamma = 0.95$.\n",
    "\n",
    "(Hint: the provided equation updates the Q value of a state based on the reward received and the maximum Q value of the next state. You'll need to switch perspectives in implementing this: update the *previous state's* Q value based on the *current state*.)\n",
    "\n",
    "(Hint: this is not necessary, but if you get stuck because you don't understand why the Q-table is defined as it is, you might want to jump ahead and look at the discussion section at the end of the notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a51aeff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_q_table(grid):\n",
    "    \"\"\"\n",
    "    Initialize a Q-table with all values set to zero.\n",
    "\n",
    "    The Q-table is a dictionary of dictionaries, where the keys of the outer dictionary identify each node,\n",
    "    and the inner dictionaries map actions (\"up\", \"down\", \"left\", \"right\") to Q-values.\n",
    "    \n",
    "    (You can ignore the loop over two sets of nodes for now.)\n",
    "    \"\"\"\n",
    "    q_table = {}\n",
    "    for nodes in [grid.nodes, grid.nodes2]:\n",
    "        for row in nodes:\n",
    "            for node in row:\n",
    "                q_table[node.id] = {\"up\": 0, \"down\": 0, \"left\": 0, \"right\": 0}\n",
    "    return q_table\n",
    "\n",
    "def update_q_value(agent, q_table, reward):\n",
    "    \"\"\"\n",
    "    Update the Q-value for the previous state based on the action taken and the received reward.\n",
    "    \"\"\"\n",
    "    previous = agent.previous_node.id\n",
    "    current = agent.current_node.id\n",
    "    action = agent.previous_action\n",
    "    # ---------- YOUR CODE HERE ----------- #\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    current_q = q_table[previous][action]\n",
    "    max_next_q = max(q_table[current].values())\n",
    "    q_table[previous][action] = current_q + alpha * (reward + gamma * max_next_q - current_q)\n",
    "    # ---------- YOUR CODE HERE ----------- #\n",
    "\n",
    "def get_action_greedy(agent, q_table):\n",
    "    \"\"\"\n",
    "    Returns the best action according to the current policy.\n",
    "    \"\"\"\n",
    "    Q = q_table[agent.current_node.id]\n",
    "    Q_max = max(Q.values())\n",
    "    best_actions = [action for action in [\"up\", \"down\", \"left\", \"right\"] if Q[action] == Q_max]\n",
    "    return random.choice(best_actions) # break ties randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69acaac1",
   "metadata": {},
   "source": [
    "Run the following cell once to train the agent. You may adjust the visualization parameters as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd2c37e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■ ■ ■ ■ ■ ■ ■ ■ ■ ■ @ ■ ■ ■ ■\n",
      "■ . . . ■ ~ ~ ~ ■ . . . ■ . ■\n",
      "■ . . . ■ ~ ~ ~ ■ . . . . . ■\n",
      "■ . . . ■ ~ ~ ~ ■ . . . ■ . ■\n",
      "■ . . . ■ ■ — ■ ■ ■ — ■ ■ . ■\n",
      "■ . . . ■ . . . T . . . ■ . ■\n",
      "■ . . . | . T . . . T . ■ . ■\n",
      "■ . . . ■ . . . T . . . ■ . ■\n",
      "■ ■ . ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ . ■\n",
      "■ . . . . . . . . . . . . . ■\n",
      "■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■\n",
      "Current iteration: 99\n",
      "Total rewards: 100\n"
     ]
    }
   ],
   "source": [
    "# ---------- YOUR CODE HERE (OPTIONAL) ----------- #\n",
    "visualize = True # toggle visualization\n",
    "increment = 1 # see every ´increment´th iteration\n",
    "delay = 0.01 # adjust as needed\n",
    "# ---------- YOUR CODE HERE (OPTIONAL) ----------- #\n",
    "\n",
    "random.seed(42548)\n",
    "agent = QLearningAgent(grid)\n",
    "q_table = initialize_q_table(grid)\n",
    "\n",
    "for i in range(100):\n",
    "    print(f\"Running iteration {i}\")\n",
    "    while True:\n",
    "        action = get_action_greedy(agent, q_table)\n",
    "\n",
    "        if action:\n",
    "            reward = agent.move(action)\n",
    "            agent.total_rewards += reward\n",
    "            update_q_value(agent, q_table, reward)\n",
    "\n",
    "        if visualize and i % increment == 0:\n",
    "            grid.visualize(agent)\n",
    "            print(f\"Current iteration: {i}\")\n",
    "            print(f\"Total rewards: {agent.total_rewards}\")\n",
    "            # ---------- YOUR CODE HERE (OPTIONAL) ----------- #\n",
    "            # Optional debugging can be added here.\n",
    "            # You may uncomment the lines below to inspect\n",
    "            # the Q-values of the current and previous nodes.\n",
    "\n",
    "            # Q = q_table[agent.current_node.id]\n",
    "            # print(\"\\nCurrent node Q-values:\")\n",
    "            # print(f\"   {Q['up']:.2f}\")\n",
    "            # print(f\"{Q['left']:.2f} {Q['right']:.2f}\")\n",
    "            # print(f\"   {Q['down']:.2f}\")\n",
    "            # prev_Q = q_table[agent.previous_node.id]\n",
    "            # print(\"\\nPrevious node Q-values:\")\n",
    "            # print(f\"   {prev_Q['up']:.2f}\")\n",
    "            # print(f\"{prev_Q['left']:.2f} {prev_Q['right']:.2f}\")\n",
    "            # print(f\"   {prev_Q['down']:.2f}\")\n",
    "            # ---------- YOUR CODE HERE (OPTIONAL) ----------- #\n",
    "            time.sleep(delay)\n",
    "\n",
    "        if agent.current_node.goal or agent.current_node.lava:\n",
    "            break\n",
    "\n",
    "    agent.reset(grid)\n",
    "    grid.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae633a0",
   "metadata": {},
   "source": [
    "Run the following cell to test your agent (can be done multiple times). It should have converged to a stable policy within 100 iterations. If the agent gets stuck in an infinite loop, there is something wrong with your implementation or reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25c2f6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■ ■ ■ ■ ■ ■ ■ ■ ■ ■ @ ■ ■ ■ ■\n",
      "■ . . . ■ ~ ~ ~ ■ . . . ■ . ■\n",
      "■ . . . ■ ~ ~ ~ ■ . . . . . ■\n",
      "■ . . . ■ ~ ~ ~ ■ . . . ■ . ■\n",
      "■ . . . ■ ■ — ■ ■ ■ — ■ ■ . ■\n",
      "■ . . . ■ . . . T . . . ■ . ■\n",
      "■ . . . | . T . . . T . ■ . ■\n",
      "■ . . . ■ . . . T . . . ■ . ■\n",
      "■ ■ . ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ . ■\n",
      "■ . . . . . . . . . . . . . ■\n",
      "■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■\n",
      "Total rewards: 100\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    action = get_action_greedy(agent, q_table)\n",
    "    if action:\n",
    "        reward = agent.move(action)\n",
    "        agent.total_rewards += reward\n",
    "        grid.visualize(agent)\n",
    "        print(f\"Total rewards: {agent.total_rewards}\")\n",
    "        time.sleep(0.1)\n",
    "    if agent.current_node.goal or agent.current_node.lava:\n",
    "        break\n",
    "\n",
    "agent.reset(grid)\n",
    "grid.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25279124",
   "metadata": {},
   "source": [
    "The learned policy should now guide the agent through the room in the middle, as this is the shortest path to the goal. However, the agent might still make a suboptimal detour near the goal.* The reason for this behavior lies in the exploration strategy being used.\n",
    "\n",
    "The `get_action_greedy` function always selects the action with the highest Q-value for the current state. This strategy maximizes *exploitation* of the current best-known action but involves no *exploration* of other potentially better actions. As a result, the agent may get stuck in local optima and fail to discover more optimal paths.\n",
    "\n",
    "<small>*This is generally non-deterministic, but has been enforced through a fixed seed for the random number generator. If you managed to define the reward function so that you already found the optimal policy, congratulations! You still need to do the final task.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a92270",
   "metadata": {},
   "source": [
    "**Task 3: Epsilon-Greedy Exploration (0.3 pt)**\n",
    "\n",
    "To encourage exploration, we can utilize a similar method as in Assignment 2 by implementing an **epsilon-greedy** strategy. This involves selecting a random action with a small probability $\\epsilon$ (e.g. 0.1) and the best-known action with probability $1 - \\epsilon$. Optionally, we could start with a higher value and decay $\\epsilon$ over time to reduce exploration as the agent becomes more confident in its learned policy, but we will not do that here.\n",
    "\n",
    "Your task is to implement the `get_action_epsilon_greedy` function to select actions according to this strategy. You may utilize the `get_action_greedy` function and the `random` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef9e3b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_epsilon_greedy(agent, epsilon, q_table):\n",
    "    # ---------- YOUR CODE HERE ----------- #\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice([\"up\", \"down\", \"left\", \"right\"])\n",
    "    else:\n",
    "        return get_action_greedy(agent, q_table)\n",
    "    # ---------- YOUR CODE HERE ----------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3799000",
   "metadata": {},
   "source": [
    "The code for training and testing your agent is the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0222fb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■ ■ ■ ■ ■ ■ ■ ■ ■ ■ @ ■ ■ ■ ■\n",
      "■ . . . ■ ~ ~ ~ ■ . . . ■ . ■\n",
      "■ . . . ■ ~ ~ ~ ■ . . . . . ■\n",
      "■ . . . ■ ~ ~ ~ ■ . . . ■ . ■\n",
      "■ . . . ■ ■ — ■ ■ ■ — ■ ■ . ■\n",
      "■ . . . ■ . . . T . . . ■ . ■\n",
      "■ . . . | . T . . . T . ■ . ■\n",
      "■ . . . ■ . . . T . . . ■ . ■\n",
      "■ ■ . ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ . ■\n",
      "■ . . . . . . . . . . . . . ■\n",
      "■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■\n",
      "Current iteration: 99\n",
      "Total rewards: 100\n"
     ]
    }
   ],
   "source": [
    "# ---------- YOUR CODE HERE (OPTIONAL) ----------- #\n",
    "visualize = True # toggle visualization\n",
    "increment = 1 # see every ´increment´th iteration\n",
    "delay = 0.00 # adjust as needed\n",
    "# ---------- YOUR CODE HERE (OPTIONAL) ----------- #\n",
    "\n",
    "random.seed(42548)\n",
    "agent = QLearningAgent(grid)\n",
    "q_table = initialize_q_table(grid)\n",
    "epsilon = 0.1\n",
    "\n",
    "for i in range(100):\n",
    "    while True:\n",
    "        action = get_action_epsilon_greedy(agent, epsilon, q_table)\n",
    "\n",
    "        if action:\n",
    "            reward = agent.move(action)\n",
    "            agent.total_rewards += reward\n",
    "            update_q_value(agent, q_table, reward)\n",
    "\n",
    "        if visualize and i % increment == 0:\n",
    "            grid.visualize(agent)\n",
    "            print(f\"Current iteration: {i}\")\n",
    "            print(f\"Total rewards: {agent.total_rewards}\")\n",
    "            # ---------- YOUR CODE HERE (OPTIONAL) ----------- #\n",
    "            # OPTIONAL DEBUGGING CAN BE ADDED HERE\n",
    "            \n",
    "            # Q = q_table[agent.current_node.id]\n",
    "            # print(\"\\nCurrent node Q-values:\")\n",
    "            # print(f\"   {Q['up']:.2f}\")\n",
    "            # print(f\"{Q['left']:.2f} {Q['right']:.2f}\")\n",
    "            # print(f\"   {Q['down']:.2f}\")\n",
    "            # prev_Q = q_table[agent.previous_node.id]\n",
    "            # print(\"\\nPrevious node Q-values:\")\n",
    "            # print(f\"   {prev_Q['up']:.2f}\")\n",
    "            # print(f\"{prev_Q['left']:.2f} {prev_Q['right']:.2f}\")\n",
    "            # print(f\"   {prev_Q['down']:.2f}\")\n",
    "            # ---------- YOUR CODE HERE (OPTIONAL) ----------- #\n",
    "            time.sleep(delay)\n",
    "\n",
    "        if agent.current_node.goal or agent.current_node.lava:\n",
    "            break\n",
    "\n",
    "    agent.reset(grid)\n",
    "    grid.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2890bc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "■ ■ ■ ■ ■ ■ ■ ■ ■ ■ @ ■ ■ ■ ■\n",
      "■ . . . ■ ~ ~ ~ ■ . . . ■ . ■\n",
      "■ . . . ■ ~ ~ ~ ■ . . . . . ■\n",
      "■ . . . ■ ~ ~ ~ ■ . . . ■ . ■\n",
      "■ . . . ■ ■ — ■ ■ ■ — ■ ■ . ■\n",
      "■ . . . ■ . . . T . . . ■ . ■\n",
      "■ . . . | . T . . . T . ■ . ■\n",
      "■ . . . ■ . . . T . . . ■ . ■\n",
      "■ ■ . ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ . ■\n",
      "■ . . . . . . . . . . . . . ■\n",
      "■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■\n",
      "Total rewards: 100\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    action = get_action_greedy(agent, q_table)\n",
    "    if action:\n",
    "        reward = agent.move(action)\n",
    "        agent.total_rewards += reward\n",
    "        grid.visualize(agent)\n",
    "        print(f\"Total rewards: {agent.total_rewards}\")\n",
    "        time.sleep(0.1)\n",
    "    if agent.current_node.goal or agent.current_node.lava:\n",
    "        break\n",
    "\n",
    "agent.reset(grid)\n",
    "grid.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f6674",
   "metadata": {},
   "source": [
    "The agent should now learn the optimal policy within 100 iterations. Notice that we used `get_action_epsilon_greedy` for training but `get_action_greedy` for testing, as we want to exploit the learned policy during testing rather than continue exploring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cbda8e",
   "metadata": {},
   "source": [
    "### EXTRA: Discussion\n",
    "\n",
    "**Something we neglected to mention before is why we're using two different sets of nodes in initializing two distinct Q-tables. The reason for this is somewhat intricate: we're actually working with two different grids that represent different states of the environment. Nodes can be thought of as (x, y, k) tuples where x and y are the coordinates and k is a boolean indicating whether the key has been collected. One grid is the original grid in which doors are locked and the key is not yet collected (k = 0), while the other grid represents all states after the key has been collected (k = 1) and the doors are unlocked. The underlying logic has been abstracted away, but the agent effectively gets transported to a whole new environment with its own set of states and corresponding Q-values after picking up the key.**\n",
    "\n",
    "**Why is this necessary? What would happen if we used only a single grid and provided a positive reward when landing on the key square and subsequently 1) didn't remove it from the grid, or 2) removed it from the grid?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea01319",
   "metadata": {},
   "source": [
    "Q-learning assumes the Markov property, that the value of being in a state depends only on that state, not on the history of how you got there. If the same (x, y) coordinate can mean two fundamentally different things depending on whether the key has been picked up, the Q-values for that coordinate become meaningless because they're trying to average over two incompatible situations:\n",
    "\n",
    "Case 1: Key stays on the grid after collection\n",
    "The key square would accumulate a positive reward every time the agent steps on it, whether or not the key has already been collected. The agent would learn to repeatedly revisit the key square to farm rewards, never progressing toward the door or goal. The Q-values around the key square would be artificially inflated, pulling the agent into a pointless loop.\n",
    "\n",
    "Case 2: Key is removed from the grid after collection\n",
    "This sounds reasonable, but now the same grid node (x, y) means something different before and after the key is picked up. A single Q-table would blend these two meanings into one entry. Early in training, Q-values are learned with the key present; later visits to the same coordinate update the same Q-values even though the strategic context has completely changed. The agent can't learn a coherent policy because the optimal action at (x, y) genuinely differs depending on whether k=0 or k=1 — for example, the correct path to the goal may require going through the now-unlocked door, which wasn't even accessible before.\n",
    "Why two grids solve this\n",
    "\n",
    "By treating the state as (x, y, k), each node in each grid has its own dedicated Q-values that are only ever updated in the correct context. The agent in the k=0 world learns to navigate toward the key; the agent in the k=1 world learns to navigate through the unlocked door toward the goal. These are genuinely different problems with different optimal policies, and keeping them separate ensures the Markov property holds and the Q-values remain meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b05c6a",
   "metadata": {},
   "source": [
    "## Aftermath\n",
    "\n",
    "Please provide short answers to the following questions:\n",
    "\n",
    "**1. Did you experience any issues or find anything particularly confusing?**\n",
    "\n",
    "No\n",
    "\n",
    "**2. Is there anything you would like to see improved in the assignment?**\n",
    "\n",
    "No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60ef1f6",
   "metadata": {},
   "source": [
    "### Submission\n",
    "\n",
    "1. Make sure you have completed all tasks and filled in your personal details at the top of this notebook.\n",
    "2. Ensure all the code runs without errors: restart the kernel and run all cells in order.\n",
    "3. Submit *only* this notebook (`ex6.ipynb`) on Moodle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
